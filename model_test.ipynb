{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import UNET\n",
    "from dataset import UNET_Dataset\n",
    "import albumentations as A\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.transforms import ToPILImage\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn.functional import sigmoid\n",
    "\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNET.load_from_checkpoint(\"lightning_logs\\\\version_9\\\\checkpoints\\\\epoch=124-step=9750.ckpt\", lr = 1e-3, weight_decay=1e-3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "data = UNET_Dataset(\"Dataset/test\", A.Compose(\n",
    "            [\n",
    "                A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "                A.Normalize(\n",
    "                    mean=[0.0, 0.0, 0.0],\n",
    "                    std=[1.0, 1.0, 1.0],\n",
    "                    max_pixel_value=255.0,\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ],\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "34",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (img, seg) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[0;32m      2\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[0;32m      3\u001b[0m     preds \u001b[38;5;241m=\u001b[39m sigmoid(output)\n",
      "File \u001b[1;32md:\\Machine Learning\\U-Net Segmentation\\dataset.py:30\u001b[0m, in \u001b[0;36mUNET_Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(idx):\n\u001b[0;32m     28\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 30\u001b[0m img_coco \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgs\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Read Image\u001b[39;00m\n\u001b[0;32m     32\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdir, img_coco[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 34"
     ]
    }
   ],
   "source": [
    "for i, (img, seg) in enumerate(data):\n",
    "    output = model(img.unsqueeze(0).cuda())\n",
    "    preds = sigmoid(output)\n",
    "    preds = (preds >= 0.5).float()\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "\n",
    "    fig.add_subplot(1, 3, 1)\n",
    "    plt.imshow(ToPILImage()(img))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Input\")\n",
    "\n",
    "    fig.add_subplot(1, 3, 2)\n",
    "    plt.imshow(ToPILImage()(seg))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Target\")\n",
    "\n",
    "    fig.add_subplot(1, 3, 3)\n",
    "    plt.imshow(ToPILImage()(preds.squeeze(0)))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Output\")\n",
    "    \n",
    "    plt.savefig(\"Outputs/\" + str(i) + \".jpg\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "WARNING:tensorflow:From c:\\Users\\makro\\.conda\\envs\\research_project\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15937a5900ca4e4eb998b10bb043c1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        accuracy            0.8956891298294067\n",
      "       dice_score            0.79505455493927\n",
      "        test_loss           0.7620352506637573\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.7620352506637573,\n",
       "  'dice_score': 0.79505455493927,\n",
       "  'accuracy': 0.8956891298294067}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_module import DataModule\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 256\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "DIR = \"D:\\\\Machine Learning\\\\U-Net Segmentation\\\\Dataset\"\n",
    "\n",
    "data_module = DataModule(DIR, NUM_WORKERS, BATCH_SIZE, PIN_MEMORY)\n",
    "data_module.setup(\"validate\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    devices=[0],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    \"D:\\\\Machine Learning\\\\U-Net Segmentation\\\\saved_models\\\\model.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
